{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b48d78",
   "metadata": {},
   "source": [
    "# 基本信息\n",
    "\n",
    "\n",
    "| 班级   | 姓名 | 学号 |\n",
    "| :------- | ------ | ------ |\n",
    "| 品牌2201班 | 陈炯儒  | 20211003020 |\n",
    "|        | 牛嘉鑫   | 20221303863  |\n",
    "|        | 林佳怡   | 20221303769  |\n",
    "|        | 章子慧   | 20221303772  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a814b61",
   "metadata": {},
   "source": [
    "# 任务： 网络中心性的度量与解释（电视剧主演）\n",
    "\n",
    "1. 构建“小组偏爱的演员网络”。根据组员的偏好，爬取豆瓣上不少于200部评分不低于7分的中国（含香港、澳门、台湾）电视剧的“主演”信息。凡出演同一部电视剧的演员之间，建立一条连边。连边上的权重代表演员之间的合作（即：共同出演同一部剧的）次数。\n",
    "\n",
    "2. 构建“官方偏爱的演员网络”，根据中国电视剧三大奖项：飞天奖、金鹰奖、白玉兰奖的获奖名单，从豆瓣爬取名单上不少于200部电视剧的“主演”信息。凡主演。。。同上。\n",
    "\n",
    "3. 用python绘制出上述两个演员合作网（有权图）\n",
    "4. 计算两个网络中，各节点的四种中心性（点度中心性，中介中心性，接近中心性，特征向量中心性），并排序、输出各结果中排在前30的演员名单极其排序。同时，解读每种中心性结果的现实意义，并比较两个网络中共同出现的演员在不同网络中，各中心性指标的得分差异【提示：跨网络比较，请使用标准化的XX中心得分】。\n",
    "\n",
    "5. 针对每种中心性度量中得分最高的5个节点，写一个循环，每次去掉一个当前得分最高的节点并输出节点排序情况。观察网络中，各节点XX中心性的排序变化情况，并在此基础上，加深理解各中心指标与网络结构之间的关系。\n",
    "\n",
    "\n",
    "# 任务要求\n",
    "\n",
    "1. 所有任务均需要提交至少两个文件：\n",
    "    1. 数据\n",
    "    2. 以Jupyter notebook提交的代码及全部运行结果。\n",
    "\n",
    "2. 文件的第一部分，请以markdown模式，标注班级、组员的姓名和学号。\n",
    "3. 文件的第二部分，请以markdown模式放置老师布置的任务描述。\n",
    "4. 文件的第三部分，请以markdown模式，阐述任务代码的编写思路（如：1.建立一个列表/字典/元祖，存入节点对之间的指向关系；2.利用XX函数，将列表/字典/元祖转化为矩阵a；3.利用XX包的XX函数，计算矩阵a的转置矩阵，并存储为矩阵b；4. ...\n",
    "5. 文件的第四部分，放置代码，并在一段代码块之前后之后，记得添加comment，说明每段代码的意图做什么【注：老师在阅读你们的代码过程中，如因缺乏注释而有不理解的地方，可能会请你们当面解释代码，再决定如何打分。】。\n",
    "6. 评分标准：能正确使用markdown（第一、第二部分），思路清晰且详尽（第三部分），代码跑得通（第四部分），结果正确且解读正确（第四部分，部分作用需要解读结果，那么，请在代码后面，以Markdown模式，加入对结果的解读）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa7d95",
   "metadata": {},
   "source": [
    "# 思路\n",
    "\n",
    "\n",
    "\n",
    "###  数据收集\n",
    "\n",
    "1. 利用豆瓣接口采集评分不低于的“国产剧”和“港剧”，不少于200部的电视剧列表。主要采集信息包括电视剧ID, 评分，并按照评分过滤出不低于7分的目标。\n",
    "2. 利用百度百科等网站人工收集白玉兰奖、飞天奖、金鹰奖等往届获奖电视剧名称。\n",
    "3. 利用豆瓣电影搜索功能，根据上述所收集的电视剧名称，采集电视剧对应的电视剧ID。\n",
    "4. 上述两类电视剧信息保存到文件中，供之后使用。\n",
    "2. 通过电视剧ID爬取到每部电视剧的主演信息，并保存。\n",
    "    1. 演员的id到姓名的映射信息。\n",
    "    2. 保存电视剧ID到演员ID列表的映射信息。\n",
    "\n",
    "### 网络构建\n",
    "\n",
    "1. 读取电视剧映射信息，并加入label来区分是小组喜欢的电视剧还是官方喜欢的电视剧。\n",
    "2. 读取演员映射信息。\n",
    "3. 读取电视剧的演员映射信息。\n",
    "\n",
    "### 数据分析\n",
    "\n",
    "1. 对不同的label (小组喜欢 vs 官方喜欢）\n",
    "    1. 从电视剧的演员映射信息计算演员合作的次数。\n",
    "    2. 利用networkx库构建网络图。`create_graph()`函数\n",
    "    3. 对构建的网络图进行可视化。`plot_graph()`函数\n",
    "    4. 对构建的网络图进行分析。`analysis()`函数\n",
    "        1. 计算点度中心性。`networkx.degree_centrality(graph)`\n",
    "        2. 计算中介中心性。`networkx.between_centrality(graph)`\n",
    "        3. 计算接近中心性。`networkx.closeness_centrality(graph)`\n",
    "        4. 计算特征向量中心性。`networkx.eigenvector_centrality(graph)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8a4df",
   "metadata": {},
   "source": [
    "# 第一阶段：数据采集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7cc71",
   "metadata": {},
   "source": [
    "## 环境准备 \n",
    "\n",
    "\n",
    "首先安装依赖的python库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4088cdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: networkx in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (3.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wenbaoli/anaconda3/envs/painterPy/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install networkx\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7ce4d",
   "metadata": {},
   "source": [
    "导入所依赖的python库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07393397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote, unquote\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e6d47",
   "metadata": {},
   "source": [
    "## 参数预定义\n",
    "\n",
    "这里主要包括以下类型的参数：\n",
    "1. 爬虫相关参数\n",
    "2. 数据读写文件参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0836bc2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 首先定义一些变量\n",
    "\n",
    "# ！！！注意\n",
    "# 1.爬虫的时候需要设置自己的cookie,否则会因为反爬虫等限制请求失败\n",
    "# 2.cookie是标识个人身份的数据，请严密保管自己的登录cookie\n",
    "# 3.cookie需要通过在网页登录账号后，通过开发者工具获取\n",
    "cookie = '''请输入自己的cookie，具体获取方法需要登录豆瓣，打开开发者工具，打开network找到一个请求，然后找到request header的cookie'''\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"Sec-Ch-Ua\": '''\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"''',\n",
    "    \"Referer\": \"https://movie.douban.com/\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"cookie\": cookie,\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "GUOCHANJUAN_TAG = \"国产剧\"\n",
    "GANGJU_TAG = \"港剧\"\n",
    "REWARD_TAG = \"三大电视奖\"\n",
    "\n",
    "REWARD_TV_FILE = \"data/reward_tv_titles.csv\"\n",
    "\n",
    "GET_DOUBAN_TV_TAG_URL = \"https://movie.douban.com/j/search_subjects\"\n",
    "SEARCH_TV_URL = \"https://www.douban.com/search?cat=1002&q=\"\n",
    "GET_ACTOR_URL = f\"https://movie.douban.com/subject/\"\n",
    "\n",
    "\n",
    "GROUP_LIKE_LABEL = 'group_like'\n",
    "OFFICIAL_LIKE_LABEL = 'official_like'\n",
    "\n",
    "# 数据文件名，数据存储在同目录的data目录里\n",
    "# REWARD_TV_FILE = \"data/reward_tv_titles.csv\"\n",
    "REWARD_TV_LIST_FILE = \"data/reward_tv_list.csv\"\n",
    "TV_LOOKUP_FILE = \"data/tv_list.csv\"\n",
    "ACTOR_LOOKUP_FILE = \"data/actor_list.csv\"\n",
    "TV_ACTORS_FILE = \"data/tv_actor_list.csv\"\n",
    "\n",
    "\n",
    "TMP_REWARD_TV_LIST_FILE = \"data/reward_tv_list.csv\"\n",
    "TMP_TV_LOOKUP_FILE = \"data/tv_list.csv\"\n",
    "TMP_ACTOR_LOOKUP_FILE = \"data/actor_list.csv\"\n",
    "TMP_TV_ACTORS_FILE = \"data/tv_actor_list.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a9662",
   "metadata": {},
   "source": [
    "## 数据采集相关函数\n",
    "\n",
    "首先我们分别分析了豆瓣电影的三个网页，利用http请求对应地址，并使用BeautifulSoup这个爬虫工具对返回的html页面进行解析，提取我们的目标数据。\n",
    "\n",
    "- 搜索电视剧\n",
    "- 电视剧详情页\n",
    "\n",
    "\n",
    "\n",
    "但在实际采集过程中，由于目标网站反爬虫机制，爬取过程可能并不是太稳定，且部分请求会比较耗时。\n",
    "\n",
    "总之，本文定义了以下几个函数，基于不同的输入参数返回目标数据，包括：\n",
    "- get_douban_tv_by_tag函数：根据指定的tag名称，爬取num条电视剧。\n",
    "- search_tv_by_name函数：根据指定的电视剧名，爬取该电视剧在豆瓣的LINK(该LINK可以解析ID)\n",
    "- get_tv_actors函数：根据指定的电视剧ID，爬取该电视剧的主演名称及ID。\n",
    "\n",
    "另外两个函数则是其他逻辑函数。用于上层爬取数据及存储数据。\n",
    "\n",
    "我们的采集顺序如下：\n",
    "1. 爬取300部国产剧，且只保留评分不低于7分的，219部。\n",
    "2. 爬取300部港剧，且只保留评分不低于7分的，234部。\n",
    "3. 人工收集三大奖项获奖电视剧200部。\n",
    "4. 将上述三类电视剧汇总去重，得到电视剧624部。\n",
    "5. 根据上述624部电视剧的ID，爬取其对应的主演信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a193f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据电视剧类型请求电视剧列表\n",
    "def get_douban_tv_by_tag(tag, num = 1):\n",
    "    # 爬取的目标地址接口\n",
    "    url = GET_DOUBAN_TV_TAG_URL\n",
    "    # 目标接口所需要的参数\n",
    "    params = {\n",
    "        \"type\": \"tv\",\n",
    "        \"tag\": tag,\n",
    "        \"page_limit\": num,\n",
    "        \"page_start\": 0\n",
    "    }\n",
    "    # 使用requests这个网络请求库来请求目标URL的数据，这注意必须指定headers，否则请求会失败\n",
    "    response = requests.get(url, params=params, headers = headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"request failed for tag:{tag}, 返回码:{response.status_code}\")\n",
    "        return []\n",
    "    # 将返回的结果转为字典，并获取电视剧列表，若请求结果不存在对应列表，则默认为空列表\n",
    "    dramas = response.json().get('subjects', [])\n",
    "\n",
    "    # 定义一个列表，用来存储要采集的电视剧信息\n",
    "    filtered_dramas = []\n",
    "    # 循环遍历电视剧列表，提取关键信息\n",
    "    for drama in dramas:\n",
    "        # 获取电视剧名称\n",
    "        title = drama[\"title\"]\n",
    "        \n",
    "        # 获取电视剧URL，并去掉末尾的/\n",
    "        url = drama[\"url\"].strip('/')\n",
    "        \n",
    "        # 获取电视剧评分，这里拿到的是字符串\n",
    "        rate = drama[\"rate\"]\n",
    "        \n",
    "        # 如果拿到是空结果，或者评分小于7，则丢弃这条数据，继续处理下一条数据\n",
    "        if rate == '' or float(rate) < 7:\n",
    "            print(f\"没有评分或者评分低于7, 电视剧：{title}, 评分:{rate}\")\n",
    "            continue\n",
    "        # 获取电视剧ID\n",
    "        did = drama[\"id\"]\n",
    "        \n",
    "        # 创建一个列表并包含上述数据，最后添加filtered_dramas这个返回变量中\n",
    "        filtered_dramas.append([tag, did, title, rate, url])\n",
    "        \n",
    "        # 下面是用来记录爬虫进度的打印信息，每采集到5个电视剧打印一条提示\n",
    "        if len(filtered_dramas) % 5 == 0:\n",
    "            print(f\"get_douban_tv_by_tag for {tag}, size {len(filtered_dramas)} dramas\")\n",
    "    # 返回采集的电视剧信息列表\n",
    "    return filtered_dramas\n",
    "\n",
    "\n",
    "# 根据电视剧名称搜索获取电视剧ID(link里包含ID信息)，返回结果为电视剧访问的link\n",
    "def search_tv_by_name(search_text):\n",
    "    \n",
    "    # 由于搜索文本可能包含特殊符号，或者是中文，因此需要先对其进行转化\n",
    "    quoted_search_text = quote(search_text)\n",
    "\n",
    "    # 构造请求的链接\n",
    "    url = f\"{SEARCH_TV_URL}{quoted_search_text}\"\n",
    "\n",
    "    # 发送请求\n",
    "    r = requests.get(url, headers=headers)\n",
    "    \n",
    "    status = r.status_code\n",
    "    # 若请求成功，则返回码是200\n",
    "    if status == 200:\n",
    "        # 构造一个BeautifulSoup解析器\n",
    "        soup = BeautifulSoup(r.content.decode('utf-8'),\"html.parser\",from_encoding=\"utf-8\")\n",
    "        \n",
    "        # 先找到class名为result-list的div标签块\n",
    "        a = soup.find('div', class_='result-list')\n",
    "        if a is None:\n",
    "            print(f\"fail soup find for {search_text}\")\n",
    "            return None\n",
    "        num = len(a)\n",
    "        if num == 0:\n",
    "            print('request succ, but null result')\n",
    "        else:\n",
    "            href = a.find('a', href=True)\n",
    "            link = href['href'].split('?url=')[-1].split('%2F&query=')[0]\n",
    "            uq_link = unquote(link).strip('/')\n",
    "            return uq_link\n",
    "    else:\n",
    "        print(f\"Request failed, code:{status}\")\n",
    "    return None\n",
    "\n",
    "# 根据电视剧ID获取其主演信息，并返回到一个列表中\n",
    "def get_tv_actors(tv_id):\n",
    "    # 构造请求链接\n",
    "    url = f\"{GET_ACTOR_URL}{tv_id}\"\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    status = r.status_code\n",
    "    return_actors = []\n",
    "    if status == 200:\n",
    "        soup = BeautifulSoup(r.content.decode('utf-8'),\"html.parser\")\n",
    "        a = soup.find('div', class_='subject clearfix')\n",
    "        if a is not None:\n",
    "            actors = a.find('span', class_='actor')\n",
    "            if actors is not None:\n",
    "                attrs = actors.find('span', class_='attrs')\n",
    "                if attrs is not None: \n",
    "                    hrefs = attrs.find_all('a', href=True)\n",
    "                    if hrefs is not None:\n",
    "                        for item in hrefs:\n",
    "                            aname = item.get_text()\n",
    "                            aid = item['href'].strip('/')\n",
    "                            return_actors.append((aname, aid))\n",
    "    else:\n",
    "        print(f\"request failed for tv_id:{tv_id}, status:{status}\")\n",
    "    return return_actors\n",
    "\n",
    "\n",
    "# 读取三大电视剧奖获得列表的电视剧名称，分别搜索其电视剧LINK/ID，并按照一定格式存储到一个列表中\n",
    "def get_reward_tv(num):\n",
    "    with open(REWARD_TV_FILE, 'r') as f:\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            title = line.strip()\n",
    "            link = search_tv_by_name(title)\n",
    "            if link is not None:\n",
    "                tv_id = link.strip().strip('/').split('/')[-1]\n",
    "                result.append([REWARD_TAG, tv_id, title, '10', link])\n",
    "            if len(result) % 5 == 0:\n",
    "                print(f\"get_reward_tv size:{len(result)} dramas\")\n",
    "            if len(result) >= num:\n",
    "                break\n",
    "        return result\n",
    "\n",
    "\n",
    "# 将定义好格式的电视剧信息列表，追加存储到文件中        \n",
    "def tv_write(data, tag, label):\n",
    "    # 这里我们用了tmp_这个文件前缀，是担心再次运行爬虫程序时候由于不稳定影响结果，可能导致之前已经爬取到的数据混乱或丢失\n",
    "    tmp_file = TMP_TV_LOOKUP_FILE\n",
    "    with open(tmp_file, 'a+') as f:\n",
    "        for tv in data:\n",
    "            [_, tv_id, title, rate, link] = tv\n",
    "            # 最后存储文件会用逗号作为字段分隔符，因此，预先将title这个字段里的逗号替换成&\n",
    "            ntitle = title.replace(',', '&')\n",
    "            simple_tv = [tv_id, ntitle, tag, label]\n",
    "            # 使用join函数将上述列表转为逗号分隔的字符串，最后写入文件中\n",
    "            f.write(','.join(simple_tv) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b39cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============测试 get_douban_tv_by_tag======\n",
      "没有评分或者评分低于7, 电视剧：神隐, 评分:\n",
      "没有评分或者评分低于7, 电视剧：鸣龙少年, 评分:\n",
      "[['国产剧', '35797771', '一念关山', '7.3', 'https://movie.douban.com/subject/35797771']]\n",
      "============测试 search_tv_by_name======\n",
      "https://movie.douban.com/subject/35797771\n",
      "============测试 get_tv_actors======\n",
      "[('刘诗诗', 'celebrity/1274533'), ('刘宇宁', 'celebrity/1401585'), ('方逸伦', 'celebrity/1359360'), ('何蓝逗', 'celebrity/1376538'), ('陈昊宇', 'celebrity/1351561'), ('常华森', 'celebrity/1437324'), ('王艳', 'celebrity/1274509'), ('吕行', 'celebrity/1313870'), ('李欢', 'celebrity/1349341'), ('陈宥维', 'celebrity/1386145'), ('陈都灵', 'celebrity/1342249'), ('王一哲', 'celebrity/1397908'), ('陈小纭', 'celebrity/1361294'), ('张芷溪', 'celebrity/1323727'), ('黄梦莹', 'celebrity/1349244'), ('张帆', 'celebrity/1420285'), ('叶青', 'celebrity/1315746'), ('叶筱玮', 'celebrity/1371589'), ('原若航', 'celebrity/1439518'), ('吴弘', 'celebrity/1342009'), ('张垒', 'celebrity/1276088'), ('苏梦芸', 'celebrity/1452637'), ('张乔耳', 'celebrity/1440942'), ('周陆啦', 'celebrity/1418957'), ('尹铸胜', 'celebrity/1313561'), ('张天阳', 'celebrity/1339958'), ('常铖', 'celebrity/1314524'), ('曾柯琅', 'celebrity/1424329'), ('景如洋', 'celebrity/1408694')]\n",
      "============测试 get_reward_tv======\n",
      "[['三大电视奖', '6539457', '奢香夫人', '10', 'https://movie.douban.com/subject/6539457']]\n"
     ]
    }
   ],
   "source": [
    "# 简单地测试一下上面几个函数\n",
    "print(\"============测试 get_douban_tv_by_tag======\")\n",
    "ta = get_douban_tv_by_tag(\"国产剧\", 3)\n",
    "print(ta)\n",
    "\n",
    "print(\"============测试 search_tv_by_name======\")\n",
    "tb = search_tv_by_name(\"一念关山\")\n",
    "print(tb)\n",
    "\n",
    "print(\"============测试 get_tv_actors======\")\n",
    "tc = get_tv_actors(\"35797771\")\n",
    "print(tc)\n",
    "\n",
    "print(\"============测试 get_reward_tv======\")\n",
    "td = get_reward_tv(1)\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd350a",
   "metadata": {},
   "source": [
    "## 爬取300部国产剧和300部港剧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取300部国产剧\n",
    "NUM = 300\n",
    "gcj_list = get_douban_tv_by_tag(GUOCHANJUAN_TAG, NUM)\n",
    "gj_list = get_douban_tv_by_tag(GANGJU_TAG, NUM)\n",
    "\n",
    "# 写电视剧id/name映射文件\n",
    "tv_write(gcj_list, GUOCHANJUAN_TAG, GROUP_LIKE_LABEL)\n",
    "tv_write(gj_list, GANGJU_TAG, GROUP_LIKE_LABEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d64f4",
   "metadata": {},
   "source": [
    "## 爬取获奖电视剧的ID信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取获奖电视剧的ID信息，本步骤耗时较长\n",
    "# 由于网站有反爬虫限制，爬取一次后存储结果，之后直接读取这个爬取结果，不再重复爬取\n",
    "reward_list = get_reward_tv(NUM)  \n",
    "\n",
    "reward_list = []\n",
    "NUM = 200\n",
    "with open(REWARD_TV_LIST_FILE, 'r') as f:\n",
    "        result = []\n",
    "        for line in f.readlines():\n",
    "            item = line.strip().split(',')\n",
    "            tv_id = item[0]\n",
    "            title = item[1]\n",
    "            link = item[2]\n",
    "            reward_list.append([REWARD_TAG, tv_id, title, '10', link])\n",
    "            if len(reward_list) % 5 == 0:\n",
    "                print(f\"get_reward_tv size:{len(reward_list)} dramas\")\n",
    "            if len(reward_list) >= NUM:\n",
    "                break\n",
    "\n",
    "tv_write(reward_list, REWARD_TAG, OFFICIAL_LIKE_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89285aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总电视剧部数：624\n"
     ]
    }
   ],
   "source": [
    "# 定义一个集合来汇总并去重上述电视剧\n",
    "tv_id_set = set()\n",
    "with open(TV_LOOKUP_FILE, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        item = line.strip().split(',')\n",
    "        tv_id_set.add(item[0])\n",
    "\n",
    "# 将集合转为列表\n",
    "tv_id_list = list(tv_id_set)\n",
    "print(f\"总电视剧部数：{len(tv_id_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03d411",
   "metadata": {},
   "source": [
    "## 爬取主演信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取上述电视剧的主演列表\n",
    "\n",
    "# 定义一个字典，存储演员ID到演员名的映射\n",
    "actor_map = {}\n",
    "# 定义一个字典，存储电视剧ID到主演们ID的映射\n",
    "tv2actor_simple = {}\n",
    "for tv_id in tv_id_list:\n",
    "    # 爬取指定电视剧ID的主演信息\n",
    "    v = get_tv_actors(tv_id)\n",
    "    # 若返回空列表，则跳过处理下一个电视剧ID\n",
    "    if len(v) == 0:\n",
    "        continue\n",
    "    \n",
    "    # 定义一个列表，存储演员ID\n",
    "    aid_list = []\n",
    "    \n",
    "    # 遍历演员列表，解析演员ID和演员名，分别存储到列表和字典中\n",
    "    for item in v:\n",
    "        [name, aid] = item\n",
    "        aid = aid.split('/')[-1]\n",
    "        actor_map[aid] = name\n",
    "        aid_list.append(aid)\n",
    "    # 将电视剧ID到主演们ID的映射存储到目标字典中\n",
    "    tv2actor_simple[tv_id] = aid_list\n",
    "    \n",
    "    # 记录爬取进度，每5个打印一次\n",
    "    if len(tv2actor_simple) % 5 == 0:\n",
    "        print(f\"get_tv_actors size:{len(tv2actor_simple)} dramas\")\n",
    "\n",
    "print(f\"actor size: {len(actor_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写演员映射表\n",
    "def actor_write(data):\n",
    "    with open(TMP_ACTOR_LOOKUP_FILE, 'w') as f:\n",
    "        for k, v in data.items():\n",
    "            v = v.replace(',', '&')\n",
    "            f.write(f'{k},{v}\\n')\n",
    "actor_write(actor_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e69cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写电视剧的演员列表\n",
    "with open(TMP_TV_ACTORS_FILE, 'w') as f:\n",
    "    for k, v in tv2actor_simple.items():\n",
    "        v_str = ' '.join(v)\n",
    "        f.write(f'{k},{v_str}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa348f",
   "metadata": {},
   "source": [
    "# 第二部分：网络构建和分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47d3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取演员列表id->name\n",
    "# 定义一个字典，用来存储演员ID到演员名称的映射\n",
    "actor_id2name = {}\n",
    "with open(ACTOR_LOOKUP_FILE, 'r') as f:\n",
    "    # 循环遍历读取文件的每一行\n",
    "    for line in f.readlines():\n",
    "        # 先按照逗号把演员ID和演员名分隔开，存储为一个列表\n",
    "        item = line.strip().split(',')\n",
    "        actor_id = item[0]\n",
    "        actor_name = item[1]\n",
    "        actor_id2name[actor_id] = actor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29adee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取电视剧列表id->title\n",
    "# 定义一个字典，用来存储电视剧ID到电视剧名称的映射\n",
    "tv_id2title = {}\n",
    "\n",
    "# 定义一个字典，用来存储小组偏爱的电视剧ID 和 官方偏爱的电视剧ID\n",
    "# 用于后续区分两个网络的节点使用\n",
    "label2tvlist = {'group_like': {}, 'official_like': {}}\n",
    "with open(TV_LOOKUP_FILE, 'r') as f:\n",
    "    # 循环遍历读取文件的每一行\n",
    "    for line in f.readlines():\n",
    "        # 先按照逗号把电视剧ID和电视剧名称等信息分隔开，存储为一个列表\n",
    "        item = line.strip().split(',')\n",
    "        # 列表的第一个元素是电视剧ID，第二个元素是电视剧名称\n",
    "        tv_id = item[0]\n",
    "        tv_name = item[1]\n",
    "        # 列表的第三个元素是标签，分别有三个值，“国产剧”， “港剧”，“三大电视奖”\n",
    "        tag = item[2]\n",
    "        # 列表的第四个元素是标记，分别有两个值，“official_like”， “group_like” 标识小组偏爱或者官方偏爱\n",
    "        label = item[3]\n",
    "        # 将结果存储在提前定义好的字典中，key:演员ID， value:演员名\n",
    "        tv_id2title[tv_id] = tv_name\n",
    "        # 将演员ID加入对应的标记字典中\n",
    "        label2tvlist[label][tv_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7b1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取电视剧的演员列表\n",
    "# 定义一个字典，用来存储电视剧ID到演员ID列表的映射\n",
    "tv2actor_id = {}\n",
    "with open(TV_ACTORS_FILE, 'r') as f:\n",
    "    # 循环遍历读取文件的每一行\n",
    "    for line in f.readlines():\n",
    "        # 先按照逗号把电视剧ID和演员ID列表分隔开，存储为一个2个元素的列表\n",
    "        [k, v] = line.strip().split(',')\n",
    "        # 然后按照空格把多个演员ID分隔开存储为列表\n",
    "        actors = v.split(' ')\n",
    "        # 将结果存储在提前定义好的字典中，key:电视剧ID， value:演员列表\n",
    "        tv2actor_id[k] = actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d450c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tv2actor_id size:617\n",
      "tv_id2title size:624\n",
      "actor_id2name size:7846\n"
     ]
    }
   ],
   "source": [
    "print(f'tv2actor_id size:{len(tv2actor_id)}')\n",
    "print(f'tv_id2title size:{len(tv_id2title)}')\n",
    "print(f'actor_id2name size:{len(actor_id2name)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93ef05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 3461 nodes and 114240 edges\n",
      "Graph with 3217 nodes and 118630 edges\n"
     ]
    }
   ],
   "source": [
    "####### task 1&2:构建网络 #########\n",
    "\n",
    "def create_graph(label):\n",
    "    # 计算同时出演电视剧的次数\n",
    "    edges = {}\n",
    "\n",
    "    tmp = set()\n",
    "    G = nx.Graph()\n",
    "    count = 1\n",
    "    for k, v in tv2actor_id.items():\n",
    "        if k not in label2tvlist[label]:\n",
    "            continue\n",
    "        for i in range(len(v)):\n",
    "            vi = v[i]\n",
    "            G.add_node(vi, name=actor_id2name[vi])\n",
    "            for j in range(len(v)):\n",
    "                if i < j:\n",
    "                    vj = v[j]\n",
    "                    G.add_node(vj, name=actor_id2name[vj])\n",
    "                    if v[i] not in edges:\n",
    "                        edges[vi] = {}\n",
    "                    if vj not in edges[vi]:\n",
    "\n",
    "                        edges[vi][vj] = 0\n",
    "                    edges[vi][vj] += 1\n",
    "        \n",
    "        # 这里为了减少计算量，我们只选取200部电视剧\n",
    "        if count >= 200:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    \n",
    "    for i in edges:\n",
    "        for j in edges[i]:\n",
    "            G.add_edge(i, j, weight=edges[i][j])\n",
    "            G.add_edge(j, i, weight=edges[i][j])\n",
    "    return G\n",
    "\n",
    "group_like_actor_network = create_graph(GROUP_LIKE_LABEL)\n",
    "print(group_like_actor_network)\n",
    "\n",
    "\n",
    "official_like_actor_network = create_graph(OFFICIAL_LIKE_LABEL)\n",
    "print(official_like_actor_network)\n",
    "\n",
    "# 把两个网络写到文件中，第一次执行的时候写文件，备份数据，后面再执行的时候可以注释掉这两行\n",
    "nx.write_weighted_edgelist(group_like_actor_network, \"data/group_like_actors.weighted.edgelist\")\n",
    "nx.write_weighted_edgelist(official_like_actor_network, \"data/official_like_actors.weighted.edgelist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e89636",
   "metadata": {},
   "source": [
    "## Task 1&2:绘制网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3608fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graph(G):\n",
    "    nx.draw_networkx(G, with_labels=False, node_size = 10, node_color = 'skyblue')\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(group_like_actor_network)\n",
    "plot_graph(official_like_actor_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94dacfc",
   "metadata": {},
   "source": [
    "# Task 4:计算各类中心性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "675fa4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算group_like 演员网络的各类中心性:\n",
      "计算degree_centrality\n",
      "计算betweenness_centrality\n",
      "计算closeness_centrality\n",
      "计算eigenvector_centrality\n",
      "计算official_like 演员网络的各类中心性:\n",
      "计算degree_centrality\n",
      "计算betweenness_centrality\n",
      "计算closeness_centrality\n",
      "计算eigenvector_centrality\n",
      "获取group_like 演员网络的各类中心性结果TOP30:\n",
      "获取official_like 演员网络的各类中心性结果TOP30:\n",
      "最后打印两个网络的中心性结果（同时保存数据到文件中，便于后续查看）\n",
      "|Degree|1|徐敏|丁勇岱|\n",
      "|Degree|2|马吟吟|王伍福|\n",
      "|Degree|3|房子斌|唐国强|\n",
      "|Degree|4|李宏磊|马晓伟|\n",
      "|Degree|5|蒋志光|刘之冰|\n",
      "|Degree|6|马国明|孙维民|\n",
      "|Degree|7|张陆|谢钢|\n",
      "|Degree|8|毛晓彤|刘奕君|\n",
      "|Degree|9|金世佳|卢奇|\n",
      "|Degree|10|陈牧扬|岳红|\n",
      "|Degree|11|张圣岳|王宏|\n",
      "|Degree|12|韩姝妹|王永泉|\n",
      "|Degree|13|魏惠文|许文广|\n",
      "|Degree|14|刘钧|徐永革|\n",
      "|Degree|15|邵卓尧|侯祥玲|\n",
      "|Degree|16|李斌|张喜前|\n",
      "|Degree|17|黄毅|夏天|\n",
      "|Degree|18|谢承颖|张龄心|\n",
      "|Degree|19|岳旸|郭连文|\n",
      "|Degree|20|佘诗曼|魏伟|\n",
      "|Degree|21|刘琳|刘敏涛|\n",
      "|Degree|22|国歌|靳东|\n",
      "|Degree|23|王红梅|张页石|\n",
      "|Degree|24|朱剑峰|冯晖|\n",
      "|Degree|25|刘美彤|柯蓝|\n",
      "|Degree|26|刘桂芳|张嘉益|\n",
      "|Degree|27|刘天龙|高鑫|\n",
      "|Degree|28|韦家雄|王凯|\n",
      "|Degree|29|朱泳腾|苏丽|\n",
      "|Degree|30|张柏嘉|丁海峰|\n",
      "|Betweenness|1|佘诗曼|丁勇岱|\n",
      "|Betweenness|2|苗侨伟|谢钢|\n",
      "|Betweenness|3|王健|蒋雯丽|\n",
      "|Betweenness|4|刘松仁|萨日娜|\n",
      "|Betweenness|5|檀健次|唐国强|\n",
      "|Betweenness|6|林夏薇|刘奕君|\n",
      "|Betweenness|7|黄智贤|张喜前|\n",
      "|Betweenness|8|沈保平|岳红|\n",
      "|Betweenness|9|徐敏|张嘉益|\n",
      "|Betweenness|10|张晓晨|刘之冰|\n",
      "|Betweenness|11|周德华|王劲松|\n",
      "|Betweenness|12|蔡少芬|王源|\n",
      "|Betweenness|13|侯岩松|王伍福|\n",
      "|Betweenness|14|李佳航|焦刚|\n",
      "|Betweenness|15|朱泳腾|王奎荣|\n",
      "|Betweenness|16|于明加|殷桃|\n",
      "|Betweenness|17|胡杏儿|何政军|\n",
      "|Betweenness|18|焦晃|濮存昕|\n",
      "|Betweenness|19|李泽锋|刘琳|\n",
      "|Betweenness|20|高雄|柯蓝|\n",
      "|Betweenness|21|米雪|王阳|\n",
      "|Betweenness|22|黑子|张凯丽|\n",
      "|Betweenness|23|韩晓|巩峥|\n",
      "|Betweenness|24|郭珍霓|于和伟|\n",
      "|Betweenness|25|陈宇琛|孙俪|\n",
      "|Betweenness|26|王星瀚|宋晓英|\n",
      "|Betweenness|27|刘钧|刘敏涛|\n",
      "|Betweenness|28|刘洁|徐百慧|\n",
      "|Betweenness|29|蒋一铭|陈宝国|\n",
      "|Betweenness|30|赵燕国彰|马晓伟|\n",
      "|Closeness|1|沈保平|丁勇岱|\n",
      "|Closeness|2|常铖|刘奕君|\n",
      "|Closeness|3|侯岩松|唐国强|\n",
      "|Closeness|4|檀健次|王永泉|\n",
      "|Closeness|5|刘琳|王伍福|\n",
      "|Closeness|6|毛晓彤|丁海峰|\n",
      "|Closeness|7|黑子|张嘉益|\n",
      "|Closeness|8|李泽锋|谢钢|\n",
      "|Closeness|9|李依晓|许文广|\n",
      "|Closeness|10|张陆|王宏|\n",
      "|Closeness|11|岳旸|张喜前|\n",
      "|Closeness|12|袁文康|岳红|\n",
      "|Closeness|13|马吟吟|冯雷|\n",
      "|Closeness|14|周德华|刘敏涛|\n",
      "|Closeness|15|焦刚|张龄心|\n",
      "|Closeness|16|朱泳腾|刘之冰|\n",
      "|Closeness|17|代文雯|马少骅|\n",
      "|Closeness|18|徐敏|马晓伟|\n",
      "|Closeness|19|蒋欣|岳旸|\n",
      "|Closeness|20|赵柯|张志坚|\n",
      "|Closeness|21|刘美彤|柯蓝|\n",
      "|Closeness|22|刘钧|王凯|\n",
      "|Closeness|23|宁文彤|萨日娜|\n",
      "|Closeness|24|佘诗曼|王伯昭|\n",
      "|Closeness|25|房子斌|程煜|\n",
      "|Closeness|26|陈牧扬|王劲松|\n",
      "|Closeness|27|霍建华|冯晖|\n",
      "|Closeness|28|冯晖|靳东|\n",
      "|Closeness|29|隋雨蒙|任帅|\n",
      "|Closeness|30|蒋一铭|魏伟|\n",
      "|Eigenvector|1|马吟吟|马晓伟|\n",
      "|Eigenvector|2|房子斌|王伍福|\n",
      "|Eigenvector|3|徐敏|刘之冰|\n",
      "|Eigenvector|4|李宏磊|唐国强|\n",
      "|Eigenvector|5|金世佳|孙维民|\n",
      "|Eigenvector|6|张圣岳|丁勇岱|\n",
      "|Eigenvector|7|韩姝妹|卢奇|\n",
      "|Eigenvector|8|李斌|侯祥玲|\n",
      "|Eigenvector|9|黄毅|苏丽|\n",
      "|Eigenvector|10|张陆|林津锋|\n",
      "|Eigenvector|11|林晓凡|赵小川|\n",
      "|Eigenvector|12|刘美彤|吕一丁|\n",
      "|Eigenvector|13|金丰|徐永革|\n",
      "|Eigenvector|14|孙斌|张页石|\n",
      "|Eigenvector|15|张磊|郭连文|\n",
      "|Eigenvector|16|武笑羽|赵凯|\n",
      "|Eigenvector|17|朱刚日尧|岳红|\n",
      "|Eigenvector|18|王骁|谢钢|\n",
      "|Eigenvector|19|张柏嘉|乌日根|\n",
      "|Eigenvector|20|于明加|夏天|\n",
      "|Eigenvector|21|王晴|高一玮|\n",
      "|Eigenvector|22|傅淼|奚望|\n",
      "|Eigenvector|23|李楠|姚扩|\n",
      "|Eigenvector|24|檀健次|葛友元|\n",
      "|Eigenvector|25|秦海璐|左佰学|\n",
      "|Eigenvector|26|章涛|牛志强|\n",
      "|Eigenvector|27|朱嘉琦|李思博|\n",
      "|Eigenvector|28|陆妍淇|尹键|\n",
      "|Eigenvector|29|张籽沐|赵波|\n",
      "|Eigenvector|30|冯兵|张哲人|\n"
     ]
    }
   ],
   "source": [
    "# 定义四个中心性的标签名\n",
    "import os\n",
    "import pandas as pd\n",
    "types = ['Degree', 'Betweenness', 'Closeness', 'Eigenvector']\n",
    "\n",
    "# 将中心性结果存储到文件中\n",
    "def save_centralities_to_file(centrality, tag, fp):\n",
    "    for idx, score in centrality.items():\n",
    "        fp.write(f'{tag},{idx},{score}\\n')\n",
    "\n",
    "# 从文件中读取中心性结果\n",
    "def read_centrality_from_file(filename):\n",
    "    # 利用pandas库的read_csv函数从文件读取数据到pandas的dataframe结构中\n",
    "    df = pd.read_csv(filename, header=None, names=['tag','id','score'])\n",
    "    \n",
    "    # 定义一个函数，获取不同类型的中心性数据，并返回一个字典，idx用来确定是Degree或者Betweenness或者其他\n",
    "    def get_target_centrality(idx):\n",
    "        part_df = df.loc[df['tag'] == types[idx]][['id', 'score']]\n",
    "        return part_df.set_index('id')['score'].to_dict()\n",
    "    \n",
    "    #获取Degree中心性\n",
    "    degree_centrality = get_target_centrality(0)\n",
    "    #获取Betweenness\n",
    "    betweenness_centrality = get_target_centrality(1)\n",
    "    #获取Closeness中心性\n",
    "    closeness_centrality = get_target_centrality(2)\n",
    "    #获取Eigenvector中心性\n",
    "    eigenvector_centrality = get_target_centrality(3)\n",
    "    return degree_centrality,betweenness_centrality,closeness_centrality,eigenvector_centrality\n",
    "    \n",
    "            \n",
    "# 计算中心性指标\n",
    "def calculate_centralities(G, filename):\n",
    "    \n",
    "    # 为了节省计算时间，如果我们之前已经计算过并且存储在文件中了，则直接从文件读取中心性结果\n",
    "    if os.path.exists(filename):\n",
    "        print(\"从文件中读取中心性结果\")\n",
    "        return read_centrality_from_file(filename)\n",
    "\n",
    "    # 若文件不存在，则创建文件\n",
    "    fp = open(filename, 'w')\n",
    "    print(\"计算degree_centrality\")\n",
    "    # 调用networkx的degree_centrality函数\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    save_centralities_to_file(degree_centrality, types[0], fp)\n",
    "    \n",
    "    \n",
    "    print(\"计算betweenness_centrality\")\n",
    "    # 调用networkx的betweenness_centrality函数\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "    save_centralities_to_file(betweenness_centrality, types[1], fp)\n",
    "    \n",
    "    \n",
    "    print(\"计算closeness_centrality\")\n",
    "    # 调用networkx的closeness_centrality函数\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    save_centralities_to_file(closeness_centrality,  types[2], fp)\n",
    "    \n",
    "    \n",
    "    print(\"计算eigenvector_centrality\")\n",
    "    # 调用networkx的eigenvector_centrality函数\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    save_centralities_to_file(eigenvector_centrality,  types[3], fp)\n",
    "    \n",
    "    # 最后确保关闭文件句柄\n",
    "    fp.close()\n",
    "    \n",
    "    return degree_centrality, betweenness_centrality, closeness_centrality, eigenvector_centrality\n",
    "\n",
    "# 根据中心性TOP的节点映射演员名\n",
    "def map_actors_name(top_actors):\n",
    "    new_top = []\n",
    "    for item in top_actors:\n",
    "        (idx, score) = item\n",
    "        new_top.append((idx, actor_id2name[idx], score))\n",
    "    return new_top\n",
    "\n",
    "# 根据中心性分数排序获取TOP30的节点\n",
    "def top_actors(centrality, n=30):\n",
    "    return sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "# 对计算的中心性结果进行处理，便于输出后查看理解\n",
    "def analysis(centralities_):\n",
    "    # 取top-30名演员，并映射其姓名\n",
    "    result = []\n",
    "    for i, centrality in enumerate(types):\n",
    "        rank = 1\n",
    "        display_ranklist = []\n",
    "        for (idx, name, score) in map_actors_name(top_actors(centralities_[i])):\n",
    "            display_ranklist.append((centrality, rank, name, idx, score))\n",
    "            rank += 1\n",
    "        result.append(display_ranklist)\n",
    "    return result\n",
    "\n",
    "print(f'计算{GROUP_LIKE_LABEL} 演员网络的各类中心性:')\n",
    "group_centralities_ = calculate_centralities(group_like_actor_network, './data/group_like_actor_network_centralities.csv')\n",
    "\n",
    "\n",
    "print(f'计算{OFFICIAL_LIKE_LABEL} 演员网络的各类中心性:')\n",
    "official_centralities_ = calculate_centralities(official_like_actor_network, './data/official_like_actor_network_centralities.csv')\n",
    "\n",
    "print(f'获取{GROUP_LIKE_LABEL} 演员网络的各类中心性结果TOP30:')\n",
    "group_top_actors = analysis(group_centralities_)\n",
    "\n",
    "print(f'获取{OFFICIAL_LIKE_LABEL} 演员网络的各类中心性结果TOP30:')\n",
    "official_top_actors = analysis(official_centralities_)\n",
    "\n",
    "g_top_30_actor_nodes = set()\n",
    "o_top_30_actor_nodes = set()\n",
    "print(\"最后打印两个网络的中心性结果（同时保存数据到文件中，便于后续查看）\")\n",
    "with open(\"data/centrality_result.csv\", 'w') as result_fp:\n",
    "    result_fp.write(f'中心性,排名,小组偏爱演员,官方偏爱演员\\n')\n",
    "    for i in range(4):\n",
    "        group_centrality = group_top_actors[i]\n",
    "        official_centrality = official_top_actors[i]\n",
    "        for j in range(30):\n",
    "            (centrality, rank, g_name, g_idx, g_score) = group_centrality[j]\n",
    "            (_, _, o_name, o_idx, o_score) = official_centrality[j]\n",
    "            g_top_30_actor_nodes.add(g_idx)\n",
    "            o_top_30_actor_nodes.add(o_idx)\n",
    "            result_fp.write(f'{centrality},{rank},{g_name},{o_name}\\n')\n",
    "            print(f'|{centrality}|{rank}|{g_name}|{o_name}|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd418a1b",
   "metadata": {},
   "source": [
    "### 结果解释\n",
    "\n",
    "首先在电视剧演员合作网络中，这四个中心性指标代表的现实意义是：\n",
    "\n",
    "1. **Degree Centrality (度中心性)**：这个指标衡量一个演员与多少其他演员有合作关系。一个度中心性高的演员意味着他/她与很多其他演员有合作，表明他/她在电视剧行业中很活跃且具有广泛的联系网。\n",
    "\n",
    "2. **Betweenness Centrality (介数中心性)**：这个指标衡量一个演员在其他演员之间合作关系的桥梁作用。高介数中心性的演员常常处于不同演员群体或社交圈的中心位置，意味着他们在连接不同演员群体方面发挥着重要作用。\n",
    "\n",
    "3. **Closeness Centrality (接近中心性)**：这个指标衡量一个演员与网络中所有其他演员的平均距离。接近中心性高的演员意味着他/她与网络中的其他演员较为接近，可以较快地与大多数演员建立联系。\n",
    "\n",
    "4. **Eigenvector Centrality (特征向量中心性)**：这个指标不仅考虑了一个演员的直接联系数量，还考虑了这些联系的重要性。如果一个演员与多个重要或中心性高的演员有联系，那么他/她的特征向量中心性也会较高。这意味着他/她不仅与众多演员有联系，而且这些联系都是影响力较大的演员。\n",
    "\n",
    "结合本课题中计算出来的各中心性结果，我们可以得出以下几个推断。\n",
    "1. 对于点度中心性来讲，\n",
    "\n",
    "| 中心性类型   | 排名 | 小组偏爱 | 官方偏爱 |\n",
    "| :------- | ------ | :----- | :----- |\n",
    "|Degree|1|徐敏|丁勇岱|\n",
    "|Degree|2|马吟吟|王伍福|\n",
    "|Degree|3|房子斌|唐国强|\n",
    "|Degree|4|李宏磊|马晓伟|\n",
    "|Degree|5|蒋志光|刘之冰|\n",
    "|Degree|6|马国明|孙维民|\n",
    "|Degree|7|张陆|谢钢|\n",
    "|Degree|8|毛晓彤|刘奕君|\n",
    "|Degree|9|金世佳|卢奇|\n",
    "|Degree|10|陈牧扬|岳红|\n",
    "\n",
    "徐敏、马吟吟、房子斌、李宏磊、将志光、马国明等人均是活跃的演员。而对官方偏爱的电视剧演员网络来讲，我们可以发现获奖的这些电视剧的演员大都是我们如今所熟悉的老戏骨，他们在各大优秀电视剧出演。他们都与很多演员建立了合作关系。\n",
    "\n",
    "2. 而介数中心性的排位，则体现了这些演员在演员合作网络中的桥梁作用。一方面这些演员能够排序在top，说明本身的演艺经历；从另一个侧面，我们也可以推断这些演员出演的电视剧类型丰富，因此他们能够连接不同演群。如我们非常熟知的佘诗曼、蒋雯丽，刘奕君等人。\n",
    "\n",
    "| 中心性类型   | 排名 | 小组偏爱 | 官方偏爱 |\n",
    "| :------- | ------ | :----- | :----- |\n",
    "| Betweenness | 1 | 佘诗曼 | 丁勇岱 |\n",
    "|Betweenness|2|苗侨伟|谢钢|\n",
    "|Betweenness|3|王健|蒋雯丽|\n",
    "|Betweenness|4|刘松仁|萨日娜|\n",
    "|Betweenness|5|檀健次|唐国强|\n",
    "|Betweenness|6|林夏薇|刘奕君|\n",
    "|Betweenness|7|黄智贤|张喜前|\n",
    "|Betweenness|8|沈保平|岳红|\n",
    "|Betweenness|9|徐敏|张嘉益|\n",
    "|Betweenness|10|张晓晨|刘之冰|\n",
    "\n",
    "3. 再来看接近中心性这个指标，说明这些演员与影响力大的演员合作密切，比如年轻人相对熟知的檀健次，刘琳，毛晓彤，李依晓，刘奕君等人。\n",
    "| 中心性类型   | 排名 | 小组偏爱 | 官方偏爱 |\n",
    "| :------- | ------ | :----- | :----- |\n",
    "|Closeness|1|沈保平|丁勇岱|\n",
    "|Closeness|2|常铖|刘奕君|\n",
    "|Closeness|3|侯岩松|唐国强|\n",
    "|Closeness|4|檀健次|王永泉|\n",
    "|Closeness|5|刘琳|王伍福|\n",
    "|Closeness|6|毛晓彤|丁海峰|\n",
    "|Closeness|7|黑子|张嘉益|\n",
    "|Closeness|8|李泽锋|谢钢|\n",
    "|Closeness|9|李依晓|许文广|\n",
    "|Closeness|10|张陆|王宏|\n",
    "\n",
    "4. 特征向量中心性: 这个指标更多可以衡量演员在这个合作网络中的综合地位。排在TOP的演员基本都是互相合作的经历会高于其他演员。\n",
    "\n",
    "| 中心性类型   | 排名 | 小组偏爱 | 官方偏爱 |\n",
    "| :------- | ------ | :----- | :----- |\n",
    "|Eigenvector|1|马吟吟|马晓伟|\n",
    "|Eigenvector|2|房子斌|王伍福|\n",
    "|Eigenvector|3|徐敏|刘之冰|\n",
    "|Eigenvector|4|李宏磊|唐国强|\n",
    "|Eigenvector|5|金世佳|孙维民|\n",
    "|Eigenvector|6|张圣岳|丁勇岱|\n",
    "|Eigenvector|7|韩姝妹|卢奇|\n",
    "|Eigenvector|8|李斌|侯祥玲|\n",
    "|Eigenvector|9|黄毅|苏丽|\n",
    "|Eigenvector|10|张陆|林津锋|\n",
    "\n",
    "\n",
    "5. 综合起来看，官方偏爱的电视剧演员合作网络的上述4个指标排名TOP10的演员变化相比小组偏爱的要小一些，一方面可以推测是因为获奖类电视剧的题材等比较固定，另一方面也可能取决于收集的数据包含很多有年代的剧目。而小组偏爱网络这边，因为我们选取了国产剧和港剧两者，采集数据时默认是按照热度。因此在四个指标中的结果上看起来会有更大变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d7fc59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对网络的各个中心性进行标准化\n"
     ]
    }
   ],
   "source": [
    "# 对比两个网络的公共结点在各自网络中的中心性分数\n",
    "\n",
    "# 首先，对两个网络的中心性结果进行标准化\n",
    "from statistics import mean, stdev\n",
    "def normalize_centrality(centralities):\n",
    "    result = []\n",
    "    for i, centrality in enumerate(types):\n",
    "        m = mean(centralities[i].values())\n",
    "        std = stdev(centralities[i].values())\n",
    "        if std == 0:\n",
    "            if m == 0:\n",
    "                result.append(centralities[i])\n",
    "            else:\n",
    "                centrality_normalized = {k: v / m for k, v in centralities[i].items()}\n",
    "                result.append(centrality_normalized)\n",
    "        else:\n",
    "            centrality_normalized = {k: (v - m) / std for k, v in centralities[i].items()}\n",
    "            result.append(centrality_normalized)\n",
    "    return result\n",
    "\n",
    "print(\"对网络的各个中心性进行标准化\")\n",
    "g_norm_central = normalize_centrality(group_centralities_)\n",
    "o_norm_central = normalize_centrality(official_centralities_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad3f80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top30_common_nodes:  {'1313991', '1327329', '1355471', '1116446'}\n",
      "基于网络的标准化中心性分数选取TOP30个节点\n",
      "Degree\t1\t徐敏\t丁勇岱\n",
      "Degree\t2\t马吟吟\t王伍福\n",
      "Degree\t3\t房子斌\t唐国强\n",
      "Degree\t4\t李宏磊\t马晓伟\n",
      "Degree\t5\t蒋志光\t刘之冰\n",
      "Degree\t6\t马国明\t孙维民\n",
      "Degree\t7\t张陆\t谢钢\n",
      "Degree\t8\t毛晓彤\t刘奕君\n",
      "Degree\t9\t金世佳\t卢奇\n",
      "Degree\t10\t陈牧扬\t岳红\n",
      "Degree\t11\t张圣岳\t王宏\n",
      "Degree\t12\t韩姝妹\t王永泉\n",
      "Degree\t13\t魏惠文\t许文广\n",
      "Degree\t14\t刘钧\t徐永革\n",
      "Degree\t15\t邵卓尧\t侯祥玲\n",
      "Degree\t16\t李斌\t张喜前\n",
      "Degree\t17\t黄毅\t夏天\n",
      "Degree\t18\t谢承颖\t张龄心\n",
      "Degree\t19\t岳旸\t郭连文\n",
      "Degree\t20\t佘诗曼\t魏伟\n",
      "Degree\t21\t刘琳\t刘敏涛\n",
      "Degree\t22\t国歌\t靳东\n",
      "Degree\t23\t王红梅\t张页石\n",
      "Degree\t24\t朱剑峰\t冯晖\n",
      "Degree\t25\t刘美彤\t柯蓝\n",
      "Degree\t26\t刘桂芳\t张嘉益\n",
      "Degree\t27\t刘天龙\t高鑫\n",
      "Degree\t28\t韦家雄\t王凯\n",
      "Degree\t29\t朱泳腾\t苏丽\n",
      "Degree\t30\t张柏嘉\t丁海峰\n",
      "Betweenness\t1\t佘诗曼\t丁勇岱\n",
      "Betweenness\t2\t苗侨伟\t谢钢\n",
      "Betweenness\t3\t王健\t蒋雯丽\n",
      "Betweenness\t4\t刘松仁\t萨日娜\n",
      "Betweenness\t5\t檀健次\t唐国强\n",
      "Betweenness\t6\t林夏薇\t刘奕君\n",
      "Betweenness\t7\t黄智贤\t张喜前\n",
      "Betweenness\t8\t沈保平\t岳红\n",
      "Betweenness\t9\t徐敏\t张嘉益\n",
      "Betweenness\t10\t张晓晨\t刘之冰\n",
      "Betweenness\t11\t周德华\t王劲松\n",
      "Betweenness\t12\t蔡少芬\t王源\n",
      "Betweenness\t13\t侯岩松\t王伍福\n",
      "Betweenness\t14\t李佳航\t焦刚\n",
      "Betweenness\t15\t朱泳腾\t王奎荣\n",
      "Betweenness\t16\t于明加\t殷桃\n",
      "Betweenness\t17\t胡杏儿\t何政军\n",
      "Betweenness\t18\t焦晃\t濮存昕\n",
      "Betweenness\t19\t李泽锋\t刘琳\n",
      "Betweenness\t20\t高雄\t柯蓝\n",
      "Betweenness\t21\t米雪\t王阳\n",
      "Betweenness\t22\t黑子\t张凯丽\n",
      "Betweenness\t23\t韩晓\t巩峥\n",
      "Betweenness\t24\t郭珍霓\t于和伟\n",
      "Betweenness\t25\t陈宇琛\t孙俪\n",
      "Betweenness\t26\t王星瀚\t宋晓英\n",
      "Betweenness\t27\t刘钧\t刘敏涛\n",
      "Betweenness\t28\t刘洁\t徐百慧\n",
      "Betweenness\t29\t蒋一铭\t陈宝国\n",
      "Betweenness\t30\t赵燕国彰\t马晓伟\n",
      "Closeness\t1\t沈保平\t丁勇岱\n",
      "Closeness\t2\t常铖\t刘奕君\n",
      "Closeness\t3\t侯岩松\t唐国强\n",
      "Closeness\t4\t檀健次\t王永泉\n",
      "Closeness\t5\t刘琳\t王伍福\n",
      "Closeness\t6\t毛晓彤\t丁海峰\n",
      "Closeness\t7\t黑子\t张嘉益\n",
      "Closeness\t8\t李泽锋\t谢钢\n",
      "Closeness\t9\t李依晓\t许文广\n",
      "Closeness\t10\t张陆\t王宏\n",
      "Closeness\t11\t岳旸\t张喜前\n",
      "Closeness\t12\t袁文康\t岳红\n",
      "Closeness\t13\t马吟吟\t冯雷\n",
      "Closeness\t14\t周德华\t刘敏涛\n",
      "Closeness\t15\t焦刚\t张龄心\n",
      "Closeness\t16\t朱泳腾\t刘之冰\n",
      "Closeness\t17\t代文雯\t马少骅\n",
      "Closeness\t18\t徐敏\t马晓伟\n",
      "Closeness\t19\t蒋欣\t岳旸\n",
      "Closeness\t20\t赵柯\t张志坚\n",
      "Closeness\t21\t刘美彤\t柯蓝\n",
      "Closeness\t22\t刘钧\t王凯\n",
      "Closeness\t23\t宁文彤\t萨日娜\n",
      "Closeness\t24\t佘诗曼\t王伯昭\n",
      "Closeness\t25\t房子斌\t程煜\n",
      "Closeness\t26\t陈牧扬\t王劲松\n",
      "Closeness\t27\t霍建华\t冯晖\n",
      "Closeness\t28\t冯晖\t靳东\n",
      "Closeness\t29\t隋雨蒙\t任帅\n",
      "Closeness\t30\t蒋一铭\t魏伟\n",
      "Eigenvector\t1\t马吟吟\t马晓伟\n",
      "Eigenvector\t2\t房子斌\t王伍福\n",
      "Eigenvector\t3\t徐敏\t刘之冰\n",
      "Eigenvector\t4\t李宏磊\t唐国强\n",
      "Eigenvector\t5\t金世佳\t孙维民\n",
      "Eigenvector\t6\t张圣岳\t丁勇岱\n",
      "Eigenvector\t7\t韩姝妹\t卢奇\n",
      "Eigenvector\t8\t李斌\t侯祥玲\n",
      "Eigenvector\t9\t黄毅\t苏丽\n",
      "Eigenvector\t10\t张陆\t林津锋\n",
      "Eigenvector\t11\t林晓凡\t赵小川\n",
      "Eigenvector\t12\t刘美彤\t吕一丁\n",
      "Eigenvector\t13\t金丰\t徐永革\n",
      "Eigenvector\t14\t孙斌\t张页石\n",
      "Eigenvector\t15\t张磊\t郭连文\n",
      "Eigenvector\t16\t武笑羽\t赵凯\n",
      "Eigenvector\t17\t朱刚日尧\t岳红\n",
      "Eigenvector\t18\t王骁\t谢钢\n",
      "Eigenvector\t19\t张柏嘉\t乌日根\n",
      "Eigenvector\t20\t于明加\t夏天\n",
      "Eigenvector\t21\t王晴\t高一玮\n",
      "Eigenvector\t22\t傅淼\t奚望\n",
      "Eigenvector\t23\t李楠\t姚扩\n",
      "Eigenvector\t24\t檀健次\t葛友元\n",
      "Eigenvector\t25\t秦海璐\t左佰学\n",
      "Eigenvector\t26\t章涛\t牛志强\n",
      "Eigenvector\t27\t朱嘉琦\t李思博\n",
      "Eigenvector\t28\t陆妍淇\t尹键\n",
      "Eigenvector\t29\t张籽沐\t赵波\n",
      "Eigenvector\t30\t冯兵\t张哲人\n",
      "Closeness\t岳旸(1327329)\t11->11\t1.4911449453391765->1.4911449453391765\n",
      "Closeness\t冯晖(1355471)\t28->28\t1.331439213376629->1.331439213376629\n"
     ]
    }
   ],
   "source": [
    "top30_common_nodes = g_top_30_actor_nodes.intersection(o_top_30_actor_nodes)\n",
    "print(\"top30_common_nodes: \", top30_common_nodes)\n",
    "\n",
    "print(\"基于网络的标准化中心性分数选取TOP30个节点\")\n",
    "norm_group_top_actors = analysis(g_norm_central)\n",
    "norm_official_top_actors = analysis(o_norm_central)\n",
    "\n",
    "\n",
    "# 打印两个网络的标准化后的中心性结果\n",
    "with open(\"data/norm_centrality_result.csv\", 'w') as result_fp:\n",
    "    result_fp.write(f'中心性,排名,小组偏爱演员,官方偏爱演员\\n')\n",
    "    for i, centrality in enumerate(types):\n",
    "        group_centrality = norm_group_top_actors[i]\n",
    "        official_centrality = norm_official_top_actors[i]\n",
    "        for j in range(30):\n",
    "            (centrality, rank, g_name, g_idx, g_score) = group_centrality[j]\n",
    "            (_, _, o_name, o_idx, o_score) = official_centrality[j]\n",
    "            result_fp.write(f'{centrality},{rank},{g_name},{o_name}\\n')\n",
    "            print(f'{centrality}\\t{rank}\\t{g_name}\\t{o_name}')\n",
    "\n",
    "\n",
    "# 获取两个网络中共同结点的排名以及中心性分数等\n",
    "g_top30_common_centrality = {}\n",
    "o_top30_common_centrality = {}\n",
    "for i, centrality in enumerate(types):\n",
    "    group_centrality = norm_group_top_actors[i]\n",
    "    official_centrality = norm_official_top_actors[i]\n",
    "    g_c_central = {}\n",
    "    o_c_central = {}\n",
    "    for j in range(30):\n",
    "        (_, _, _, g_idx, _) = group_centrality[j]\n",
    "        (_, _, _, o_idx, _) = official_centrality[j]\n",
    "        if g_idx in top30_common_nodes:\n",
    "            g_c_central[g_idx] = group_centrality[j]\n",
    "        if o_idx in top30_common_nodes:\n",
    "            o_c_central[o_idx] = official_centrality[j]\n",
    "    g_top30_common_centrality[centrality] = g_c_central\n",
    "    o_top30_common_centrality[centrality] = o_c_central\n",
    "\n",
    "# 打印对比\n",
    "for i, centrality in enumerate(types):\n",
    "    group_centrality = norm_group_top_actors[i]\n",
    "    official_centrality = norm_official_top_actors[i]\n",
    "    g_c_central = g_top30_common_centrality[centrality]\n",
    "    o_c_central = o_top30_common_centrality[centrality]\n",
    "    nodes = set(g_c_central.keys()).intersection(set(o_c_central.keys()))\n",
    "    for idx in nodes:\n",
    "        (centrality, g_rank, g_name, g_idx, g_score) = g_c_central[idx]\n",
    "        (o_centrality, o_rank, _, _, o_score) = g_c_central[idx]\n",
    "        print(f'{centrality}\\t{g_name}({g_idx})\\t{g_rank}->{o_rank}\\t{g_score}->{o_score}')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4b58f",
   "metadata": {},
   "source": [
    "从上述结果来看，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892aeb81",
   "metadata": {},
   "source": [
    "## Task 5:循环移除得分最高的节点，观察网络top结点的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "df5d1920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小组喜爱演员网络分析------------------------------->\n",
      "['1313546', '1352249', '1316392', '1342986', '1315056']\n",
      "=============Degree=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,徐敏 -> 马吟吟 -> 房子斌 -> 蒋志光 -> 蒋志光\n",
      "2,马吟吟 -> 房子斌 -> 李宏磊 -> 李宏磊 -> 马国明\n",
      "3,房子斌 -> 李宏磊 -> 蒋志光 -> 马国明 -> 张陆\n",
      "4,李宏磊 -> 蒋志光 -> 马国明 -> 张陆 -> 毛晓彤\n",
      "5,蒋志光 -> 马国明 -> 张陆 -> 毛晓彤 -> 金世佳\n",
      "6,马国明 -> 张陆 -> 毛晓彤 -> 金世佳 -> 陈牧扬\n",
      "7,张陆 -> 毛晓彤 -> 金世佳 -> 陈牧扬 -> 魏惠文\n",
      "8,毛晓彤 -> 金世佳 -> 陈牧扬 -> 魏惠文 -> 张圣岳\n",
      "9,金世佳 -> 陈牧扬 -> 魏惠文 -> 张圣岳 -> 韩姝妹\n",
      "10,陈牧扬 -> 张圣岳 -> 张圣岳 -> 韩姝妹 -> 刘钧\n",
      "11,张圣岳 -> 韩姝妹 -> 韩姝妹 -> 刘钧 -> 邵卓尧\n",
      "12,韩姝妹 -> 魏惠文 -> 刘钧 -> 邵卓尧 -> 李斌\n",
      "13,魏惠文 -> 刘钧 -> 邵卓尧 -> 李斌 -> 黄毅\n",
      "14,刘钧 -> 邵卓尧 -> 李斌 -> 黄毅 -> 岳旸\n",
      "15,邵卓尧 -> 李斌 -> 黄毅 -> 岳旸 -> 佘诗曼\n",
      "16,李斌 -> 黄毅 -> 岳旸 -> 佘诗曼 -> 谢承颖\n",
      "17,黄毅 -> 岳旸 -> 谢承颖 -> 谢承颖 -> 刘琳\n",
      "18,谢承颖 -> 谢承颖 -> 佘诗曼 -> 刘琳 -> 国歌\n",
      "19,岳旸 -> 佘诗曼 -> 刘琳 -> 国歌 -> 王红梅\n",
      "20,佘诗曼 -> 刘琳 -> 国歌 -> 王红梅 -> 朱剑峰\n",
      "21,刘琳 -> 国歌 -> 王红梅 -> 朱剑峰 -> 刘美彤\n",
      "22,国歌 -> 王红梅 -> 朱剑峰 -> 刘美彤 -> 刘桂芳\n",
      "23,王红梅 -> 朱剑峰 -> 刘美彤 -> 刘桂芳 -> 刘天龙\n",
      "24,朱剑峰 -> 刘美彤 -> 刘桂芳 -> 刘天龙 -> 韦家雄\n",
      "25,刘美彤 -> 刘桂芳 -> 刘天龙 -> 韦家雄 -> 朱泳腾\n",
      "26,刘桂芳 -> 刘天龙 -> 韦家雄 -> 朱泳腾 -> 韩马利\n",
      "27,刘天龙 -> 韦家雄 -> 朱泳腾 -> 韩马利 -> 张柏嘉\n",
      "28,韦家雄 -> 朱泳腾 -> 张柏嘉 -> 张柏嘉 -> 林晓凡\n",
      "29,朱泳腾 -> 张柏嘉 -> 林晓凡 -> 林晓凡 -> 蔡国庆\n",
      "30,张柏嘉 -> 林晓凡 -> 韩马利 -> 蔡国庆 -> 袁镇业\n",
      "['1019718', '1115415', 'subject_search?search_text=%E7%8E%8B%E5%81%A5', '1028689', '1321055']\n",
      "=============Betweenness=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,佘诗曼 -> 苗侨伟 -> 林夏薇 -> 林夏薇 -> 林夏薇\n",
      "2,苗侨伟 -> 林夏薇 -> 刘松仁 -> 米雪 -> 米雪\n",
      "3,王健 -> 刘松仁 -> 米雪 -> 黄智贤 -> 黄智贤\n",
      "4,刘松仁 -> 王健 -> 黄智贤 -> 刘松仁 -> 陈杰\n",
      "5,檀健次 -> 黄智贤 -> 王健 -> 陈杰 -> 韩晓\n",
      "6,林夏薇 -> 米雪 -> 沈保平 -> 韩晓 -> 周德华\n",
      "7,黄智贤 -> 沈保平 -> 周德华 -> 沈保平 -> 赵燕国彰\n",
      "8,沈保平 -> 周德华 -> 张晓晨 -> 周德华 -> 郑嘉颖\n",
      "9,徐敏 -> 徐敏 -> 蔡少芬 -> 赵燕国彰 -> 蔡少芬\n",
      "10,张晓晨 -> 于明加 -> 于明加 -> 张晓晨 -> 张晓晨\n",
      "11,周德华 -> 张晓晨 -> 徐敏 -> 于明加 -> 胡杏儿\n",
      "12,蔡少芬 -> 胡杏儿 -> 胡杏儿 -> 蔡少芬 -> 沈保平\n",
      "13,侯岩松 -> 蔡少芬 -> 郭珍霓 -> 胡杏儿 -> 于明加\n",
      "14,李佳航 -> 李佳航 -> 高雄 -> 徐敏 -> 徐敏\n",
      "15,朱泳腾 -> 朱泳腾 -> 李佳航 -> 高雄 -> 高雄\n",
      "16,于明加 -> 高雄 -> 朱泳腾 -> 李佳航 -> 周海媚\n",
      "17,胡杏儿 -> 韩晓 -> 周海媚 -> 周海媚 -> 李佳航\n",
      "18,焦晃 -> 郭珍霓 -> 韩晓 -> 焦晃 -> 陈键锋\n",
      "19,李泽锋 -> 焦晃 -> 焦晃 -> 陈键锋 -> 檀健次\n",
      "20,高雄 -> 檀健次 -> 檀健次 -> 檀健次 -> 马国明\n",
      "21,米雪 -> 王星瀚 -> 陈键锋 -> 蒋志光 -> 韦家雄\n",
      "22,黑子 -> 刘洁 -> 蒋志光 -> 韦家雄 -> 蒋志光\n",
      "23,韩晓 -> 陈宇琛 -> 韦家雄 -> 刘洁 -> 蒋欣\n",
      "24,郭珍霓 -> 陈键锋 -> 马国明 -> 秦海璐 -> 刘家辉\n",
      "25,陈宇琛 -> 蒋一铭 -> 蒋欣 -> 蒋欣 -> 焦晃\n",
      "26,王星瀚 -> 蒋志光 -> 郑嘉颖 -> 马国明 -> 黑子\n",
      "27,刘钧 -> 毛晓彤 -> 刘洁 -> 刘家辉 -> 秦海璐\n",
      "28,刘洁 -> 刘钧 -> 毛晓彤 -> 黑子 -> 陈道明\n",
      "29,蒋一铭 -> 马国明 -> 陈宇琛 -> 陈宇琛 -> 毛晓彤\n",
      "30,赵燕国彰 -> 蒋欣 -> 陈道明 -> 陈道明 -> 陈宇琛\n",
      "['1319363', '1314524', '1328605', '1321055', '1116446']\n",
      "=============Closeness=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,沈保平 -> 常铖 -> 檀健次 -> 檀健次 -> 李泽锋\n",
      "2,常铖 -> 檀健次 -> 侯岩松 -> 李泽锋 -> 毛晓彤\n",
      "3,侯岩松 -> 侯岩松 -> 李泽锋 -> 毛晓彤 -> 岳旸\n",
      "4,檀健次 -> 李泽锋 -> 李依晓 -> 岳旸 -> 袁文康\n",
      "5,刘琳 -> 李依晓 -> 黑子 -> 袁文康 -> 代文雯\n",
      "6,毛晓彤 -> 黑子 -> 毛晓彤 -> 李依晓 -> 周德华\n",
      "7,黑子 -> 毛晓彤 -> 刘琳 -> 黑子 -> 张陆\n",
      "8,李泽锋 -> 刘琳 -> 岳旸 -> 代文雯 -> 刘琳\n",
      "9,李依晓 -> 岳旸 -> 袁文康 -> 徐敏 -> 蒋欣\n",
      "10,张陆 -> 袁文康 -> 周德华 -> 周德华 -> 赵柯\n",
      "11,岳旸 -> 周德华 -> 朱泳腾 -> 张陆 -> 刘钧\n",
      "12,袁文康 -> 朱泳腾 -> 代文雯 -> 刘琳 -> 宁文彤\n",
      "13,马吟吟 -> 代文雯 -> 徐敏 -> 蒋欣 -> 李依晓\n",
      "14,周德华 -> 徐敏 -> 张陆 -> 赵柯 -> 朱泳腾\n",
      "15,焦刚 -> 张陆 -> 蒋欣 -> 刘美彤 -> 黑子\n",
      "16,朱泳腾 -> 蒋欣 -> 赵柯 -> 刘钧 -> 陈牧扬\n",
      "17,代文雯 -> 赵柯 -> 刘美彤 -> 马吟吟 -> 冯晖\n",
      "18,徐敏 -> 刘美彤 -> 刘钧 -> 宁文彤 -> 隋雨蒙\n",
      "19,蒋欣 -> 刘钧 -> 马吟吟 -> 朱泳腾 -> 徐敏\n",
      "20,赵柯 -> 马吟吟 -> 佘诗曼 -> 房子斌 -> 蒋一铭\n",
      "21,刘美彤 -> 宁文彤 -> 宁文彤 -> 陈牧扬 -> 霍建华\n",
      "22,刘钧 -> 佘诗曼 -> 房子斌 -> 冯晖 -> 苗侨伟\n",
      "23,宁文彤 -> 房子斌 -> 陈牧扬 -> 隋雨蒙 -> 刘亦菲\n",
      "24,佘诗曼 -> 陈牧扬 -> 焦刚 -> 蒋一铭 -> 杨昆\n",
      "25,房子斌 -> 焦刚 -> 冯晖 -> 佘诗曼 -> 李纯\n",
      "26,陈牧扬 -> 冯晖 -> 隋雨蒙 -> 霍建华 -> 陈道明\n",
      "27,霍建华 -> 隋雨蒙 -> 蒋一铭 -> 于明加 -> 焦刚\n",
      "28,冯晖 -> 蒋一铭 -> 霍建华 -> 苗侨伟 -> 马吟吟\n",
      "29,隋雨蒙 -> 霍建华 -> 于明加 -> 刘亦菲 -> 白宇\n",
      "30,蒋一铭 -> 于明加 -> 苗侨伟 -> 杨昆 -> 于明加\n",
      "['1352249', '1316392', '1313546', '1342986', '1313839']\n",
      "=============Eigenvector=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,马吟吟 -> 房子斌 -> 徐敏 -> 李宏磊 -> 金世佳\n",
      "2,房子斌 -> 徐敏 -> 李宏磊 -> 金世佳 -> 张圣岳\n",
      "3,徐敏 -> 李宏磊 -> 金世佳 -> 张圣岳 -> 韩姝妹\n",
      "4,李宏磊 -> 金世佳 -> 张圣岳 -> 韩姝妹 -> 李斌\n",
      "5,金世佳 -> 张圣岳 -> 韩姝妹 -> 李斌 -> 黄毅\n",
      "6,张圣岳 -> 韩姝妹 -> 李斌 -> 黄毅 -> 张陆\n",
      "7,韩姝妹 -> 李斌 -> 黄毅 -> 张陆 -> 林晓凡\n",
      "8,李斌 -> 黄毅 -> 张陆 -> 林晓凡 -> 刘美彤\n",
      "9,黄毅 -> 张陆 -> 林晓凡 -> 刘美彤 -> 金丰\n",
      "10,张陆 -> 林晓凡 -> 刘美彤 -> 金丰 -> 孙斌\n",
      "11,林晓凡 -> 刘美彤 -> 金丰 -> 孙斌 -> 张磊\n",
      "12,刘美彤 -> 金丰 -> 孙斌 -> 张磊 -> 张柏嘉\n",
      "13,金丰 -> 孙斌 -> 张磊 -> 张柏嘉 -> 武笑羽\n",
      "14,孙斌 -> 张磊 -> 武笑羽 -> 武笑羽 -> 朱刚日尧\n",
      "15,张磊 -> 武笑羽 -> 朱刚日尧 -> 朱刚日尧 -> 王骁\n",
      "16,武笑羽 -> 朱刚日尧 -> 王骁 -> 王骁 -> 于明加\n",
      "17,朱刚日尧 -> 王骁 -> 张柏嘉 -> 于明加 -> 王晴\n",
      "18,王骁 -> 张柏嘉 -> 于明加 -> 王晴 -> 傅淼\n",
      "19,张柏嘉 -> 于明加 -> 王晴 -> 傅淼 -> 李楠\n",
      "20,于明加 -> 王晴 -> 傅淼 -> 李楠 -> 檀健次\n",
      "21,王晴 -> 傅淼 -> 李楠 -> 檀健次 -> 秦海璐\n",
      "22,傅淼 -> 李楠 -> 檀健次 -> 秦海璐 -> 章涛\n",
      "23,李楠 -> 檀健次 -> 秦海璐 -> 章涛 -> 蒋龙\n",
      "24,檀健次 -> 秦海璐 -> 章涛 -> 朱嘉琦 -> 孙立韬\n",
      "25,秦海璐 -> 章涛 -> 朱嘉琦 -> 陆妍淇 -> 芦宏\n",
      "26,章涛 -> 朱嘉琦 -> 陆妍淇 -> 张籽沐 -> 安亚\n",
      "27,朱嘉琦 -> 陆妍淇 -> 张籽沐 -> 冯兵 -> 许歌\n",
      "28,陆妍淇 -> 张籽沐 -> 冯兵 -> 柳明明 -> 曾柯琅\n",
      "29,张籽沐 -> 冯兵 -> 柳明明 -> 米咪 -> 冷海铭\n",
      "30,冯兵 -> 柳明明 -> 米咪 -> 蔡珩 -> 张燕\n",
      "官方喜爱演员网络分析------------------------------->\n",
      "['1314975', '1314011', '1152771', '1274973', '1316135']\n",
      "=============Degree=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,丁勇岱 -> 王伍福 -> 唐国强 -> 马晓伟 -> 刘之冰\n",
      "2,王伍福 -> 唐国强 -> 马晓伟 -> 刘之冰 -> 孙维民\n",
      "3,唐国强 -> 马晓伟 -> 刘之冰 -> 孙维民 -> 谢钢\n",
      "4,马晓伟 -> 刘之冰 -> 孙维民 -> 谢钢 -> 刘奕君\n",
      "5,刘之冰 -> 孙维民 -> 谢钢 -> 刘奕君 -> 卢奇\n",
      "6,孙维民 -> 谢钢 -> 刘奕君 -> 卢奇 -> 王宏\n",
      "7,谢钢 -> 刘奕君 -> 卢奇 -> 王宏 -> 岳红\n",
      "8,刘奕君 -> 卢奇 -> 王宏 -> 岳红 -> 王永泉\n",
      "9,卢奇 -> 岳红 -> 岳红 -> 王永泉 -> 许文广\n",
      "10,岳红 -> 王宏 -> 王永泉 -> 许文广 -> 徐永革\n",
      "11,王宏 -> 王永泉 -> 许文广 -> 徐永革 -> 张喜前\n",
      "12,王永泉 -> 许文广 -> 徐永革 -> 张喜前 -> 侯祥玲\n",
      "13,许文广 -> 徐永革 -> 张喜前 -> 侯祥玲 -> 张龄心\n",
      "14,徐永革 -> 侯祥玲 -> 侯祥玲 -> 张龄心 -> 夏天\n",
      "15,侯祥玲 -> 张喜前 -> 夏天 -> 夏天 -> 魏伟\n",
      "16,张喜前 -> 夏天 -> 张龄心 -> 魏伟 -> 郭连文\n",
      "17,夏天 -> 张龄心 -> 魏伟 -> 郭连文 -> 刘敏涛\n",
      "18,张龄心 -> 郭连文 -> 郭连文 -> 刘敏涛 -> 靳东\n",
      "19,郭连文 -> 魏伟 -> 刘敏涛 -> 靳东 -> 柯蓝\n",
      "20,魏伟 -> 刘敏涛 -> 靳东 -> 张页石 -> 冯晖\n",
      "21,刘敏涛 -> 靳东 -> 张页石 -> 柯蓝 -> 张页石\n",
      "22,靳东 -> 张页石 -> 柯蓝 -> 冯晖 -> 张嘉益\n",
      "23,张页石 -> 柯蓝 -> 冯晖 -> 张嘉益 -> 高鑫\n",
      "24,冯晖 -> 冯晖 -> 张嘉益 -> 高鑫 -> 王凯\n",
      "25,柯蓝 -> 张嘉益 -> 高鑫 -> 王凯 -> 王劲松\n",
      "26,张嘉益 -> 高鑫 -> 王凯 -> 王劲松 -> 苏丽\n",
      "27,高鑫 -> 王凯 -> 王劲松 -> 苏丽 -> 丁海峰\n",
      "28,王凯 -> 苏丽 -> 苏丽 -> 丁海峰 -> 刘涛\n",
      "29,苏丽 -> 丁海峰 -> 丁海峰 -> 刘涛 -> 林津锋\n",
      "30,丁海峰 -> 王劲松 -> 林津锋 -> 林津锋 -> 赵小川\n",
      "['1314975', '1313180', '1020487', '1274702', '1152771']\n",
      "=============Betweenness=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,丁勇岱 -> 谢钢 -> 蒋雯丽 -> 刘奕君 -> 刘奕君\n",
      "2,谢钢 -> 蒋雯丽 -> 唐国强 -> 唐国强 -> 唐国强\n",
      "3,蒋雯丽 -> 萨日娜 -> 萨日娜 -> 萨日娜 -> 濮存昕\n",
      "4,萨日娜 -> 唐国强 -> 刘奕君 -> 濮存昕 -> 张喜前\n",
      "5,唐国强 -> 刘奕君 -> 岳红 -> 张喜前 -> 岳红\n",
      "6,刘奕君 -> 岳红 -> 张喜前 -> 岳红 -> 刘之冰\n",
      "7,张喜前 -> 张喜前 -> 刘之冰 -> 刘之冰 -> 王伍福\n",
      "8,岳红 -> 张嘉益 -> 张嘉益 -> 张嘉益 -> 王千源\n",
      "9,张嘉益 -> 刘之冰 -> 王伍福 -> 王千源 -> 张嘉益\n",
      "10,刘之冰 -> 王劲松 -> 王源 -> 王伍福 -> 焦刚\n",
      "11,王劲松 -> 王伍福 -> 焦刚 -> 焦刚 -> 王源\n",
      "12,王源 -> 王源 -> 王劲松 -> 王源 -> 王劲松\n",
      "13,王伍福 -> 焦刚 -> 王奎荣 -> 王劲松 -> 王奎荣\n",
      "14,焦刚 -> 王奎荣 -> 何政军 -> 王奎荣 -> 李阳\n",
      "15,王奎荣 -> 濮存昕 -> 濮存昕 -> 刘敏涛 -> 刘敏涛\n",
      "16,殷桃 -> 何政军 -> 殷桃 -> 何政军 -> 何政军\n",
      "17,何政军 -> 殷桃 -> 刘琳 -> 殷桃 -> 柯蓝\n",
      "18,濮存昕 -> 刘琳 -> 巩峥 -> 张凯丽 -> 张凯丽\n",
      "19,刘琳 -> 柯蓝 -> 柯蓝 -> 巩峥 -> 殷桃\n",
      "20,柯蓝 -> 王阳 -> 王阳 -> 柯蓝 -> 尤靖茹\n",
      "21,王阳 -> 巩峥 -> 刘敏涛 -> 王阳 -> 巩峥\n",
      "22,张凯丽 -> 张凯丽 -> 张凯丽 -> 于和伟 -> 王阳\n",
      "23,巩峥 -> 于和伟 -> 于和伟 -> 马晓伟 -> 马晓伟\n",
      "24,于和伟 -> 孙俪 -> 孙俪 -> 刘琳 -> 于和伟\n",
      "25,孙俪 -> 刘敏涛 -> 宋晓英 -> 宋晓英 -> 刘琳\n",
      "26,宋晓英 -> 宋晓英 -> 马晓伟 -> 白志迪 -> 陈宝国\n",
      "27,刘敏涛 -> 白志迪 -> 白志迪 -> 陈宝国 -> 白志迪\n",
      "28,徐百慧 -> 王伯昭 -> 王伯昭 -> 孙俪 -> 宋晓英\n",
      "29,陈宝国 -> 马晓伟 -> 徐百慧 -> 王伯昭 -> 孙俪\n",
      "30,马晓伟 -> 徐百慧 -> 陈宝国 -> 徐百慧 -> 王伯昭\n",
      "['1314975', '1322731', '1152771', '1348415', '1314011']\n",
      "=============Closeness=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,丁勇岱 -> 刘奕君 -> 唐国强 -> 王永泉 -> 张嘉益\n",
      "2,刘奕君 -> 唐国强 -> 王永泉 -> 张嘉益 -> 丁海峰\n",
      "3,唐国强 -> 王永泉 -> 张嘉益 -> 丁海峰 -> 王宏\n",
      "4,王永泉 -> 丁海峰 -> 丁海峰 -> 王宏 -> 谢钢\n",
      "5,王伍福 -> 王伍福 -> 许文广 -> 谢钢 -> 许文广\n",
      "6,丁海峰 -> 张嘉益 -> 王宏 -> 许文广 -> 张喜前\n",
      "7,张嘉益 -> 许文广 -> 谢钢 -> 张喜前 -> 冯雷\n",
      "8,谢钢 -> 王宏 -> 岳红 -> 冯雷 -> 岳红\n",
      "9,许文广 -> 谢钢 -> 张喜前 -> 岳红 -> 马少骅\n",
      "10,王宏 -> 张喜前 -> 冯雷 -> 马少骅 -> 张志坚\n",
      "11,张喜前 -> 岳红 -> 马少骅 -> 张志坚 -> 柯蓝\n",
      "12,岳红 -> 冯雷 -> 张志坚 -> 柯蓝 -> 岳旸\n",
      "13,冯雷 -> 刘之冰 -> 柯蓝 -> 岳旸 -> 刘敏涛\n",
      "14,刘敏涛 -> 马少骅 -> 岳旸 -> 刘敏涛 -> 王凯\n",
      "15,张龄心 -> 张志坚 -> 刘敏涛 -> 王凯 -> 萨日娜\n",
      "16,刘之冰 -> 柯蓝 -> 王凯 -> 萨日娜 -> 王伍福\n",
      "17,马少骅 -> 马晓伟 -> 王伍福 -> 王伍福 -> 刘之冰\n",
      "18,马晓伟 -> 岳旸 -> 萨日娜 -> 刘之冰 -> 冯晖\n",
      "19,岳旸 -> 王凯 -> 刘之冰 -> 冯晖 -> 张龄心\n",
      "20,张志坚 -> 刘敏涛 -> 程煜 -> 张龄心 -> 王劲松\n",
      "21,柯蓝 -> 张龄心 -> 王伯昭 -> 王劲松 -> 程煜\n",
      "22,王凯 -> 萨日娜 -> 冯晖 -> 程煜 -> 王伯昭\n",
      "23,萨日娜 -> 程煜 -> 张龄心 -> 王伯昭 -> 陈宝国\n",
      "24,王伯昭 -> 冯晖 -> 王劲松 -> 陈宝国 -> 魏伟\n",
      "25,程煜 -> 王伯昭 -> 陈宝国 -> 魏伟 -> 李光复\n",
      "26,王劲松 -> 王劲松 -> 魏伟 -> 李光复 -> 白志迪\n",
      "27,冯晖 -> 任帅 -> 李光复 -> 白志迪 -> 陆毅\n",
      "28,靳东 -> 魏伟 -> 白志迪 -> 陆毅 -> 高鑫\n",
      "29,任帅 -> 陈宝国 -> 陆毅 -> 高鑫 -> 任帅\n",
      "30,魏伟 -> 李光复 -> 高鑫 -> 任帅 -> 黄俊鹏\n",
      "['1274973', '1314011', '1316135', '1152771', '1316990']\n",
      "=============Eigenvector=============\n",
      "rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5\n",
      "1,马晓伟 -> 王伍福 -> 刘之冰 -> 唐国强 -> 丁勇岱\n",
      "2,王伍福 -> 刘之冰 -> 唐国强 -> 孙维民 -> 孙维民\n",
      "3,刘之冰 -> 唐国强 -> 孙维民 -> 丁勇岱 -> 卢奇\n",
      "4,唐国强 -> 孙维民 -> 丁勇岱 -> 卢奇 -> 侯祥玲\n",
      "5,孙维民 -> 丁勇岱 -> 卢奇 -> 侯祥玲 -> 苏丽\n",
      "6,丁勇岱 -> 卢奇 -> 侯祥玲 -> 苏丽 -> 林津锋\n",
      "7,卢奇 -> 侯祥玲 -> 苏丽 -> 林津锋 -> 赵小川\n",
      "8,侯祥玲 -> 苏丽 -> 林津锋 -> 赵小川 -> 吕一丁\n",
      "9,苏丽 -> 林津锋 -> 赵小川 -> 吕一丁 -> 徐永革\n",
      "10,林津锋 -> 赵小川 -> 吕一丁 -> 徐永革 -> 张页石\n",
      "11,赵小川 -> 吕一丁 -> 徐永革 -> 张页石 -> 赵凯\n",
      "12,吕一丁 -> 徐永革 -> 张页石 -> 赵凯 -> 岳红\n",
      "13,徐永革 -> 张页石 -> 赵凯 -> 郭连文 -> 郭连文\n",
      "14,张页石 -> 郭连文 -> 郭连文 -> 岳红 -> 夏天\n",
      "15,郭连文 -> 赵凯 -> 岳红 -> 谢钢 -> 谢钢\n",
      "16,赵凯 -> 岳红 -> 谢钢 -> 夏天 -> 高一玮\n",
      "17,岳红 -> 谢钢 -> 夏天 -> 高一玮 -> 奚望\n",
      "18,谢钢 -> 夏天 -> 高一玮 -> 奚望 -> 姚扩\n",
      "19,乌日根 -> 高一玮 -> 奚望 -> 姚扩 -> 左佰学\n",
      "20,夏天 -> 乌日根 -> 姚扩 -> 左佰学 -> 牛志强\n",
      "21,高一玮 -> 奚望 -> 乌日根 -> 牛志强 -> 乌日根\n",
      "22,奚望 -> 姚扩 -> 左佰学 -> 乌日根 -> 葛友元\n",
      "23,姚扩 -> 葛友元 -> 牛志强 -> 葛友元 -> 李思博\n",
      "24,葛友元 -> 左佰学 -> 葛友元 -> 李思博 -> 尹键\n",
      "25,左佰学 -> 牛志强 -> 李思博 -> 尹键 -> 赵波\n",
      "26,牛志强 -> 李思博 -> 尹键 -> 赵波 -> 张哲人\n",
      "27,李思博 -> 尹键 -> 赵波 -> 张哲人 -> 张倬闻\n",
      "28,尹键 -> 赵波 -> 张哲人 -> 张倬闻 -> 魏子涵\n",
      "29,赵波 -> 张哲人 -> 张倬闻 -> 魏子涵 -> 何政军\n",
      "30,张哲人 -> 张倬闻 -> 魏子涵 -> 何政军 -> 刘一江\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义一个函数来移除得分最高的节点并输出节点排序情况\n",
    "def remove_top_nodes_and_rank(network, specified):\n",
    "    ranks = []\n",
    "    G = network.copy()\n",
    "    centrality = None\n",
    "    if specified == 'Degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif specified == 'Betweenness':\n",
    "        centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "    elif specified == 'Closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif specified == 'Eigenvector':\n",
    "        centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    # 排序节点，获取得分最高的节点\n",
    "    sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranks.append([actor_id2name[node] for node, _ in sorted_nodes][:30])\n",
    "    top_nodes = [node for node, _ in sorted_nodes][:5]\n",
    "    print(top_nodes)\n",
    "    for i in range(5):\n",
    "        # 移除得分最高的节点\n",
    "        G.remove_node(top_nodes[i])\n",
    "        # 计算中心性\n",
    "        centrality = None\n",
    "        if specified == 'Degree':\n",
    "            centrality = nx.degree_centrality(G)\n",
    "        elif specified == 'Betweenness':\n",
    "            centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "        elif specified == 'Closeness':\n",
    "            centrality = nx.closeness_centrality(G)\n",
    "        elif specified == 'Eigenvector':\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "        # 排序节点\n",
    "        sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "        # 保存当前的排序\n",
    "        ranks.append([actor_id2name[node] for node, _ in sorted_nodes[:30]])\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def analysis_specified_centrality(network, specified):\n",
    "# 计算并保存每种中心性指标下的节点排序变化\n",
    "    centrality_ranks = remove_top_nodes_and_rank(network, specified)\n",
    "    print(f'============={specified}=============')\n",
    "    print(f'rank, loop1 -> loop2 -> loop3 -> loop4 -> loop5')\n",
    "    for i in range(30):\n",
    "        print(f'{i+1},{centrality_ranks[0][i]} -> {centrality_ranks[1][i]} -> {centrality_ranks[2][i]} -> {centrality_ranks[3][i]} -> {centrality_ranks[4][i]}')\n",
    "\n",
    "\n",
    "def analysis_network(network):\n",
    "    for specified in types:\n",
    "        analysis_specified_centrality(network, specified)\n",
    "\n",
    "print(f'小组喜爱演员网络分析------------------------------->')      \n",
    "analysis_network(group_like_actor_network)\n",
    "\n",
    "print(f'官方喜爱演员网络分析------------------------------->')  \n",
    "analysis_network(official_like_actor_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b55d73",
   "metadata": {},
   "source": [
    "对于degree_centrality排名来说，每次去掉一个top结点后，普遍地变化是之后的节点的排名上升一位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c9353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
